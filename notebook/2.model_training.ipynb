{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Model Training\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load the raw data.\n",
    "2. Apply the preprocessing steps defined in the EDA phase.\n",
    "3. Train the Models.\n",
    "4. Evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/raw/Telco_customer_churn.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "We will apply the cleaning steps discovered during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fix Total Charges\n",
    "df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')\n",
    "df['Total Charges'] = df['Total Charges'].fillna(0)\n",
    "\n",
    "# 2. Drop Unnecessary Columns\n",
    "drop_cols = ['CustomerID', 'Count', 'Country', 'State', 'City', \n",
    "             'Zip Code', 'Lat Long', 'Latitude', 'Longitude', \n",
    "             'Churn Label', 'Churn Score', 'CLTV', 'Churn Reason']\n",
    "\n",
    "df_clean = df.drop(columns=drop_cols)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_clean.drop('Churn Value', axis=1)\n",
    "y = df_clean['Churn Value']\n",
    "\n",
    "print(f\"Original Shape: {df.shape}\")\n",
    "print(f\"Feature Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Pipeline (ColumnTransformer)\n",
    "We will use `ColumnTransformer` to apply `StandardScaler` to numerical columns and `OneHotEncoder` to categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "num_cols = ['Tenure Months', 'Monthly Charges', 'Total Charges']\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names after encoding\n",
    "feature_names = num_cols + preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "# Convert sparse matrix to dense array for Keras (if necessary)\n",
    "if hasattr(X_train_processed, 'toarray'):\n",
    "    X_train_dense = X_train_processed.toarray()\n",
    "    X_test_dense = X_test_processed.toarray()\n",
    "else:\n",
    "    X_train_dense = X_train_processed\n",
    "    X_test_dense = X_test_processed\n",
    "\n",
    "print(\"Preprocessing Complete!\")\n",
    "print(f\"Processed Feature Shape: {X_train_dense.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model: Logistic Regression (Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = log_model.predict(X_test_processed)\n",
    "\n",
    "print(\"Logistic Regression (Scaled) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Train\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_rf = rf_model.predict(X_test_processed)\n",
    "\n",
    "# 4. Evaluate\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Class Imbalance (SMOTE)\n",
    "Our previous models had low Recall for Churners (Class 1). We will use SMOTE to generate synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# 2. Resample the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Original Training Shape: {y_train.value_counts()}\")\n",
    "print(f\"Resampled Training Shape: {y_train_smote.value_counts()}\")\n",
    "\n",
    "# 3. Retrain Random Forest on Balanced Data\n",
    "rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred_smote = rf_smote.predict(X_test_processed)\n",
    "\n",
    "# 5. Evaluate\n",
    "print(\"Random Forest (SMOTE) Accuracy:\", accuracy_score(y_test, y_pred_smote))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_smote))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gradient Boosting: XGBoost (Tuned)\n",
    "We will now tune XGBoost hyperparameters to maximize Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. Define the parameter grid\n",
    "param_grid = {\n",
    "    'scale_pos_weight': [1, 3, 5],  # Key for imbalance\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "# 2. Initialize XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# 3. Setup GridSearchCV (Optimizing for Recall)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',  # Focus on catching churners\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 4. Train with GridSearch (Using SMOTE data)\n",
    "print(\"Starting Grid Search... This may take a minute.\")\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# 5. Best Parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# 6. Evaluate Best Model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_xgb_tuned = best_xgb.predict(X_test_processed)\n",
    "\n",
    "print(\"\\nXGBoost (Tuned) Accuracy:\", accuracy_score(y_test, y_pred_xgb_tuned))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb_tuned))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deep Learning: Neural Network (Keras)\n",
    "Let's build a Neural Network to see if it can capture non-linear patterns better than tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Class Weights (to handle imbalance)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "# 2. Build the Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_dense.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))  # Regularization\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# 3. Compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "# 4. Train\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_dense, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5. Evaluate\n",
    "y_pred_nn_prob = model.predict(X_test_dense)\n",
    "y_pred_nn = (y_pred_nn_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis\n",
    "Let's see which features the model thinks are most important for predicting churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances (using Random Forest for now as it's easier to interpret directly)\n",
    "importances = rf_smote.feature_importances_\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))\n",
    "plt.title('Top 10 Drivers of Churn (Random Forest + SMOTE)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('churn_model.h5')\n",
    "print(\"Model saved as churn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "class KerasWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
    "    def score(self, X, y):\n",
    "        return self.model.evaluate(X, y, verbose=0)[1]\n",
    "\n",
    "wrapped_model = KerasWrapper(model)\n",
    "\n",
    "results = permutation_importance(wrapped_model, X_test_dense, y_test, n_repeats=10, random_state=42, n_jobs=1)\n",
    "\n",
    "importance_df_nn = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': results.importances_mean\n",
    "})\n",
    "importance_df_nn = importance_df_nn.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df_nn.head(10))\n",
    "plt.title('Top 10 Drivers of Churn (Neural Network Permutation Importance)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
